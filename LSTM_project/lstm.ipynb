{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = {0:'bonafide', 1:'spoof'}\n",
    "DATA_PATH = r\"..\\spectrograms\"\n",
    "DATA_PATH =r\"..\\ASVspoof2021_DF_eval\\flac\"\n",
    "SAMPLE_RATE = 16000\n",
    "DATASET_SPOOF_SIZE = 5000\n",
    "DATASET_BONAFIDE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_df = pd.read_csv('..\\ASVspoof2021_DF_eval\\spoof_data.csv')\n",
    "inital_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count = inital_df['spoof_bonafide'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "if DATASET_BONAFIDE_SIZE > data_count['bonafide']:\n",
    "    raise Exception('ERROR: Not enough bonafide files')\n",
    "if DATASET_SPOOF_SIZE > data_count['spoof']:\n",
    "    raise Exception('ERROR: Not enough spoof files')\n",
    "\n",
    "bonafide_df = inital_df.loc[inital_df['spoof_bonafide'] == 'bonafide']\n",
    "spoof_df = inital_df.loc[inital_df['spoof_bonafide'] == 'spoof']\n",
    "\n",
    "sample_indices_bonafide = random.sample(list(bonafide_df.index), DATASET_BONAFIDE_SIZE)\n",
    "sample_indices_spoof = random.sample(list(spoof_df.index), DATASET_SPOOF_SIZE)\n",
    "\n",
    "sample_df_bonafide = inital_df.loc[sample_indices_bonafide]\n",
    "sample_indices_spoof = inital_df.loc[sample_indices_spoof]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([sample_df_bonafide, sample_indices_spoof], ignore_index=True)\n",
    "df['spoof_bonafide'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrograms = []\n",
    "signals = []\n",
    "for i, file_name in enumerate(data.recording_ID):\n",
    "    file_path = os.path.join(os.getcwd(),DATA_PATH,file_name)+'.flac'\n",
    "    audio, sample_rate = librosa.load(file_path, duration=6, sr=SAMPLE_RATE)\n",
    "    signal = np.zeros((int(SAMPLE_RATE*6,)))\n",
    "    signal[:len(audio)] = audio\n",
    "    signals.append(signal)\n",
    "    print(\"\\r Processed {}/{} files\".format(i+1,len(data)),end='')\n",
    "signals = np.stack(signals,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace({'spoof_bonafide': {'bonafide':0, 'spoof':1}})\n",
    "labels = data.spoof_bonafide.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = signals\n",
    "y = labels\n",
    "X_train, X_test_val, Y_train, Y_test_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test_val, Y_test_val, test_size=0.333, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train =  np.array(Y_train)\n",
    "Y_test =  np.array(Y_test)\n",
    "Y_val =  np.array(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n",
    "    signal_len = len(signal)\n",
    "    # Generate White Gaussian noise\n",
    "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
    "    # Normalize signal and noise\n",
    "    norm_constant = 2.0**(num_bits-1)\n",
    "    signal_norm = signal / norm_constant\n",
    "    noise_norm = noise / norm_constant\n",
    "    # Compute signal and noise power\n",
    "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
    "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
    "    # Random SNR: Uniform [15, 30] in dB\n",
    "    target_snr = np.random.randint(snr_low, snr_high)\n",
    "    # Compute K (covariance matrix) for each noise \n",
    "    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n",
    "    K = np.ones((signal_len, augmented_num)) * K  \n",
    "    # Generate noisy signal\n",
    "    return signal + K.T * noise\n",
    "aug_signals = []\n",
    "aug_labels = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    signal = X_train[i,:]\n",
    "    augmented_signals = addAWGN(signal)\n",
    "    for j in range(augmented_signals.shape[0]):\n",
    "        aug_labels.append(data.loc[i,\"spoof_bonafide\"])\n",
    "        aug_signals.append(augmented_signals[j,:])\n",
    "        data = pd.concat([data, data.iloc[i]], ignore_index=True)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "aug_signals = np.stack(aug_signals,axis=0)\n",
    "X_train = np.concatenate([X_train,aug_signals],axis=0)\n",
    "aug_labels = np.stack(aug_labels,axis=0)\n",
    "Y_train = np.concatenate([Y_train,aug_labels])\n",
    "print('')\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMELspectrogram(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
    "                                              sr=sample_rate,\n",
    "                                              n_fft=1024,\n",
    "                                              win_length = 512,\n",
    "                                              window='hamming',\n",
    "                                              hop_length = 256,\n",
    "                                              n_mels=128,\n",
    "                                              fmax=sample_rate/2\n",
    "                                             )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "# file_name = data['recording_ID'][2]\n",
    "# file_path = os.path.join(os.getcwd(),DATA_PATH,file_name)+'.flac'\n",
    "# audio, sample_rate = librosa.load(file_path, duration=6 ,sr=SAMPLE_RATE)\n",
    "# signal = np.zeros((int(SAMPLE_RATE*6,)))\n",
    "# signal[:len(audio)] = audio\n",
    "# mel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\n",
    "# librosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\n",
    "# print('MEL spectrogram shape: ',mel_spectrogram.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_train = []\n",
    "print(\"Calculatin mel spectrograms for train set\")\n",
    "for i in range(X_train.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_train.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i+1,X_train.shape[0]),end='')\n",
    "print('')\n",
    "del X_train\n",
    "\n",
    "mel_val = []\n",
    "print(\"Calculatin mel spectrograms for val set\")\n",
    "for i in range(X_val.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_val.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i+1,X_val.shape[0]),end='')\n",
    "print('')\n",
    "del X_val\n",
    "\n",
    "mel_test = []\n",
    "print(\"Calculatin mel spectrograms for test set\")\n",
    "for i in range(X_test.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_test.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i+1,X_test.shape[0]),end='')\n",
    "print('')\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitIntoChunks(mel_spec,win_size,stride):\n",
    "    t = mel_spec.shape[1]\n",
    "    num_of_chunks = int(t/stride)\n",
    "    chunks = []\n",
    "    for i in range(num_of_chunks):\n",
    "        chunk = mel_spec[:,i*stride:i*stride+win_size]\n",
    "        if chunk.shape[1] == win_size:\n",
    "            chunks.append(chunk)\n",
    "    return np.stack(chunks,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chunks\n",
    "# train set\n",
    "mel_train_chunked = []\n",
    "for mel_spec in mel_train:\n",
    "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
    "    mel_train_chunked.append(chunks)\n",
    "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
    "# val set\n",
    "mel_val_chunked = []\n",
    "for mel_spec in mel_val:\n",
    "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
    "    mel_val_chunked.append(chunks)\n",
    "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
    "# test set\n",
    "mel_test_chunked = []\n",
    "for mel_spec in mel_test:\n",
    "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
    "    mel_test_chunked.append(chunks)\n",
    "print(\"Number of chunks is {}\".format(chunks.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# BATCH FIRST TimeDistributed layer\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        # squash samples and timesteps into a single axis\n",
    "        elif len(x.size()) == 3: # (samples, timesteps, inp1)\n",
    "            x_reshape = x.contiguous().view(-1, x.size(2))  # (samples * timesteps, inp1)\n",
    "        elif len(x.size()) == 4: # (samples,timesteps,inp1,inp2)\n",
    "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3)) # (samples*timesteps,inp1,inp2)\n",
    "        else: # (samples,timesteps,inp1,inp2,inp3)\n",
    "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3),x.size(4)) # (samples*timesteps,inp1,inp2,inp3)\n",
    "            \n",
    "        y = self.module(x_reshape)\n",
    "        \n",
    "        # we have to reshape Y\n",
    "        if len(x.size()) == 3:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(1))  # (samples, timesteps, out1)\n",
    "        elif len(x.size()) == 4:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2)) # (samples, timesteps, out1,out2)\n",
    "        else:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2),y.size(3)) # (samples, timesteps, out1,out2, out3)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__()\n",
    "        # conv block\n",
    "        self.conv2Dblock = nn.Sequential(\n",
    "            # 1. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=1,\n",
    "                                   out_channels=16,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(16)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=2, stride=2)),\n",
    "            TimeDistributed(nn.Dropout(p=0.4)),\n",
    "            # 2. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=16,\n",
    "                                   out_channels=32,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(32)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
    "            TimeDistributed(nn.Dropout(p=0.4)),\n",
    "            # 3. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=32,\n",
    "                                   out_channels=64,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(64)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
    "            TimeDistributed(nn.Dropout(p=0.4)),\n",
    "            # 4. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=64,\n",
    "                                   out_channels=128,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(128)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
    "            TimeDistributed(nn.Dropout(p=0.4))\n",
    "        )\n",
    "        # LSTM block\n",
    "        hidden_size = 64\n",
    "        self.lstm = nn.LSTM(input_size=128,hidden_size=hidden_size,bidirectional=False, batch_first=True) \n",
    "        self.dropout_lstm = nn.Dropout(p=0.3)\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(hidden_size,num_emotions)\n",
    "    def forward(self,x):\n",
    "        conv_embedding = self.conv2Dblock(x)\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=2) # do not flatten batch dimension and time\n",
    "        lstm_embedding, (h,c) = self.lstm(conv_embedding)\n",
    "        lstm_embedding = self.dropout_lstm(lstm_embedding)\n",
    "        # lstm_embedding (batch, time, hidden_size)\n",
    "        lstm_output = lstm_embedding[:,-1,:] \n",
    "        output_logits = self.out_linear(lstm_output)\n",
    "        output_softmax = nn.functional.softmax(output_logits,dim=1)\n",
    "        return output_logits, output_softmax\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fnc(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fnc, optimizer):\n",
    "    def train_step(X,Y):\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        # forward pass\n",
    "        output_logits, output_softmax = model(X)\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        # compute loss\n",
    "        loss = loss_fnc(output_logits, Y)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters and zero gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,loss_fnc):\n",
    "    def validate(X,Y):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output_logits, output_softmax = model(X)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            loss = loss_fnc(output_logits,Y)\n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack(mel_train_chunked,axis=0)\n",
    "X_train = np.expand_dims(X_train,2)\n",
    "print('Shape of X_train: ',X_train.shape)\n",
    "X_val = np.stack(mel_val_chunked,axis=0)\n",
    "X_val = np.expand_dims(X_val,2)\n",
    "print('Shape of X_val: ',X_val.shape)\n",
    "X_test = np.stack(mel_test_chunked,axis=0)\n",
    "X_test = np.expand_dims(X_test,2)\n",
    "print('Shape of X_test: ',X_test.shape)\n",
    "\n",
    "del mel_train_chunked\n",
    "del mel_train\n",
    "del mel_val_chunked\n",
    "del mel_val\n",
    "del mel_test_chunked\n",
    "del mel_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "b,t,c,h,w = X_train.shape\n",
    "X_train = np.reshape(X_train, newshape=(b,-1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, newshape=(b,t,c,h,w))\n",
    "\n",
    "b,t,c,h,w = X_test.shape\n",
    "X_test = np.reshape(X_test, newshape=(b,-1))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = np.reshape(X_test, newshape=(b,t,c,h,w))\n",
    "\n",
    "b,t,c,h,w = X_val.shape\n",
    "X_val = np.reshape(X_val, newshape=(b,-1))\n",
    "X_val = scaler.transform(X_val)\n",
    "X_val = np.reshape(X_val, newshape=(b,t,c,h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=50\n",
    "DATASET_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device is {}'.format(device))\n",
    "model = HybridModel(num_emotions=len(TYPES)).to(device)\n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
    "validate = make_validate_fnc(model,loss_fnc)\n",
    "losses=[]\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # schuffle data\n",
    "    ind = np.random.permutation(DATASET_SIZE)\n",
    "    X_train = X_train[ind,:,:,:,:]\n",
    "    Y_train = Y_train[ind]\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    iters = int(DATASET_SIZE / BATCH_SIZE)\n",
    "    for i in range(iters):\n",
    "        batch_start = i * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n",
    "        actual_batch_size = batch_end-batch_start\n",
    "        X = X_train[batch_start:batch_end,:,:,:,:]\n",
    "        Y = Y_train[batch_start:batch_end]\n",
    "        X_tensor = torch.tensor(X,device=device).float()\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "        loss, acc = train_step(X_tensor,Y_tensor)\n",
    "        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n",
    "        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n",
    "        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n",
    "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
    "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
    "    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print('')\n",
    "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(os.getcwd(),'models')\n",
    "os.makedirs('models',exist_ok=True)\n",
    "torch.save(model.state_dict(),os.path.join(SAVE_PATH,'cnn_lstm_model.pt'))\n",
    "print('Model is saved to {}'.format(os.path.join(SAVE_PATH,'cnn_lstm_model.pt')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = os.path.join(os.getcwd(),'models')\n",
    "model = HybridModel(len(TYPES))\n",
    "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'cnn_lstm_model.pt')))\n",
    "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'cnn_lstm_model.pt')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
    "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
    "test_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\n",
    "print(f'Test loss is {test_loss:.3f}')\n",
    "print(f'Test accuracy is {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"spectrograms\"\n",
    "DATA_PATH =r\"pliki\\ASVspoof2021_DF_eval\\flac\"\n",
    "images = os.listdir(os.path.join(os.getcwd(),r\"spectrograms\"))\n",
    "from PIL import Image\n",
    "xdd2 = os.path.join(os.getcwd(),r\"spectrograms\",images[5])\n",
    "print(xdd2)\n",
    "im=Image.open(xdd2) \n",
    "img=np.array(im)\n",
    "wav=librosa.feature.inverse.mel_to_audio(xdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
